{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca59efeb",
   "metadata": {},
   "source": [
    "# Full Training Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dede9548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('C:/Users/admin/Desktop/Nowy folder (3)'),\n",
       " WindowsPath('C:/Users/admin/Desktop/Nowy folder (3)/TRAIN'),\n",
       " WindowsPath('C:/Users/admin/Desktop/Nowy folder (3)/TEST'),\n",
       " WindowsPath('C:/Users/admin/Desktop/Nowy folder (3)/OUTPUTS'))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path().resolve()\n",
    "TRAIN_DIR = BASE_DIR / \"TRAIN\"\n",
    "TEST_DIR  = BASE_DIR / \"TEST\"\n",
    "OUT_BASE  = BASE_DIR / \"OUTPUTS\"\n",
    "\n",
    "for p in (TRAIN_DIR, TEST_DIR, OUT_BASE):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BASE_DIR, TRAIN_DIR, TEST_DIR, OUT_BASE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f733705",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25906997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, random, csv\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Bidirectional, Conv1D, Dense, GRU, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8273ab",
   "metadata": {},
   "source": [
    "## Konfiguracja GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1ca24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tf] GPUs: 2 — memory_growth ON\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        for g in gpus:\n",
    "            try:\n",
    "                tf.config.set_memory_growth(g, True)\n",
    "            except Exception:\n",
    "                pass\n",
    "        print(f\"[tf] GPUs: {len(gpus)} — memory_growth ON\")\n",
    "    else:\n",
    "        print(\"[tf] GPU not found\")\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff99994",
   "metadata": {},
   "source": [
    "## Konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a340c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========= KONFIG – NAGŁÓWKI POD TWOJE CSV =========\n",
    "INPUT_COLUMNS: List[str] = [\n",
    "    \"feature_01\",\"feature_02\",\"feature_03\",\"feature_04\",\"feature_05\",\"feature_06\",\n",
    "    \"feature_07\",\"feature_08\",\"feature_09\",\"feature_10\",\"feature_11\",\"feature_12\",\n",
    "    \"comp_active\"\n",
    "]\n",
    "OUTPUT_COLUMN   = \"target\"\n",
    "ACTIVE_COMP_COL = \"comp_active\"   # sygnał aktywności kompresora\n",
    "\n",
    "# Cechy kategoryczne (jeżeli masz takie kolumny w INPUT_COLUMNS, dopisz je tutaj):\n",
    "CATEGORICAL_COLS: List[str] = []\n",
    "\n",
    "# Wierność/oraz zachowanie\n",
    "STRICT_SCHEMA       = True   # True: brak kolumny -> błąd; False: brakujące wypełnij 0.0\n",
    "USE_ONEHOT          = True   # włącz/wyłącz OneHot\n",
    "ROLLING_WINDOW      = 500    # stałe okno do rolling mean, jak w oryginale\n",
    "LOSS                = \"mse\"  # \"mse\" lub \"huber\"\n",
    "ACTIVE_WEIGHT_ALPHA = 0.0    # >0 aby dociążyć aktywne próbki (np. 0.5)\n",
    "\n",
    "# Opcjonalna jawna lista plików testowych (puste = bierz wszystkie z TEST/)\n",
    "TEST_FILES_OVERRIDE: List[str] = []  # np. [\"test_A.csv\",\"test_B.csv\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cf7c4",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ddb826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed: int = 50) -> None:\n",
    "    random.seed(seed); np.random.seed(seed); tf.random.set_seed(seed)\n",
    "    print(f\"[seed] {seed}\")\n",
    "\n",
    "def ensure_dir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def make_session_dir(base: Path) -> Path:\n",
    "    ensure_dir(base)\n",
    "    d = base / datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    return d\n",
    "\n",
    "def save_feature_config(cols_in: List[str], col_out: str, out_dir: Path) -> None:\n",
    "    (out_dir / \"feature_config.json\").write_text(\n",
    "        json.dumps({\"input_columns\": cols_in, \"output_column\": col_out}, indent=2),\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "def save_scalers(scaler: StandardScaler, out_scaler: StandardScaler, encoders: Dict, out_dir: Path) -> None:\n",
    "    joblib.dump(scaler, out_dir / \"scaler.gz\")\n",
    "    joblib.dump(out_scaler, out_dir / \"output_scaler.gz\")\n",
    "    joblib.dump(encoders, out_dir / \"encoders.gz\")\n",
    "\n",
    "def read_csv_auto(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Spróbuj wykryć delimiter, w razie niepowodzenia fallback na ';' i na domyślne.\"\"\"\n",
    "    try:\n",
    "        sample = path.read_text(encoding=\"utf-8\", errors=\"ignore\")[:10000]\n",
    "        dialect = csv.Sniffer().sniff(sample, delimiters=[\",\",\";\",\"|\",\"\\t\"])\n",
    "        df = pd.read_csv(path, sep=dialect.delimiter)\n",
    "        if df.shape[1] == 1 and \";\" in sample:\n",
    "            df = pd.read_csv(path, sep=\";\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            df = pd.read_csv(path, sep=\";\")\n",
    "        except Exception:\n",
    "            df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "def _init_onehot_encoder() -> OneHotEncoder:\n",
    "    # Zgodność z różnymi wersjami sklearn\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1db26",
   "metadata": {},
   "source": [
    "## Normalizacja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070833fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def normalize_data(\n",
    "    df: pd.DataFrame,\n",
    "    expected_input_cols: List[str],\n",
    "    output_col: str,\n",
    "    scaler: Optional[StandardScaler] = None,\n",
    "    out_scaler: Optional[StandardScaler] = None,\n",
    "    encoders: Optional[Dict] = None,\n",
    "    fit_encoders: bool = False,\n",
    ") -> Tuple[Optional[np.ndarray], Optional[np.ndarray], StandardScaler, StandardScaler, Dict]:\n",
    "    if df is None or df.empty:\n",
    "        return None, None, scaler, out_scaler, encoders\n",
    "\n",
    "    # Walidacja / uzupełnienie kolumn\n",
    "    missing = [c for c in expected_input_cols if c not in df.columns]\n",
    "    if missing and STRICT_SCHEMA:\n",
    "        raise ValueError(f\"Brak kolumn w danych: {missing}\")\n",
    "    for c in missing:\n",
    "        df[c] = 0.0  # łagodny fallback gdy STRICT_SCHEMA=False\n",
    "\n",
    "    # kolejność jak w expected_input_cols\n",
    "    use_cols = expected_input_cols.copy()\n",
    "    if output_col in df.columns and output_col not in use_cols:\n",
    "        use_cols.append(output_col)\n",
    "    # Zapewnij, że kolumny są w pożądanej kolejności\n",
    "    cols_final = [c for c in use_cols if c in df.columns]\n",
    "    if output_col in df.columns and output_col not in cols_final:\n",
    "        cols_final.append(output_col)\n",
    "    df = df[cols_final + ([output_col] if output_col in df.columns and output_col not in cols_final else [])]\n",
    "\n",
    "    # num vs cat\n",
    "    num_cols = [c for c in expected_input_cols if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    cat_cols = [c for c in expected_input_cols if c not in num_cols]\n",
    "    for c in CATEGORICAL_COLS:\n",
    "        if c in expected_input_cols and c not in cat_cols:\n",
    "            cat_cols.append(c)\n",
    "            if c in num_cols:\n",
    "                num_cols.remove(c)\n",
    "\n",
    "    # NA handling\n",
    "    for c in num_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(df[c].mean())\n",
    "    for c in cat_cols:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].astype(\"object\").fillna(method=\"ffill\")\n",
    "\n",
    "    scaler     = scaler or StandardScaler()\n",
    "    out_scaler = out_scaler or StandardScaler()\n",
    "    encoders   = encoders or {\"onehot\": _init_onehot_encoder()}\n",
    "\n",
    "    X_num = scaler.fit_transform(df[num_cols]) if not hasattr(scaler,\"mean_\") else scaler.transform(df[num_cols])\n",
    "    X_cat = np.empty((len(df), 0))\n",
    "    if USE_ONEHOT and cat_cols:\n",
    "        if fit_encoders or not hasattr(encoders[\"onehot\"], \"categories_\"):\n",
    "            encoders[\"onehot\"].fit(df[cat_cols])\n",
    "        X_cat = encoders[\"onehot\"].transform(df[cat_cols])\n",
    "\n",
    "    X = np.concatenate([X_num, X_cat], axis=1).astype(np.float32)\n",
    "    X = np.expand_dims(X, axis=-1)  # (n, features, 1)\n",
    "\n",
    "    if output_col in df.columns:\n",
    "        y_df = df[[output_col]]\n",
    "    else:\n",
    "        y_df = pd.DataFrame(np.zeros((len(df),1)), columns=[output_col])\n",
    "    y = out_scaler.fit_transform(y_df.values) if not hasattr(out_scaler,\"mean_\") else out_scaler.transform(y_df.values)\n",
    "    return X, y, scaler, out_scaler, encoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464abe7",
   "metadata": {},
   "source": [
    "## Model i trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e53c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(num_features: int) -> Sequential:\n",
    "    m = Sequential([\n",
    "        Conv1D(64, 1, activation=\"relu\", input_shape=(num_features, 1)),\n",
    "        Conv1D(128, 1, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Conv1D(64, 1, activation=\"relu\"),\n",
    "        Dropout(0.2),\n",
    "        Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(1e-3))),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(GRU(64, return_sequences=False, kernel_regularizer=l2(1e-3))),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation=\"linear\"),\n",
    "    ])\n",
    "    loss_fn = \"mse\" if LOSS.lower() == \"mse\" else tf.keras.losses.Huber()\n",
    "    m.compile(optimizer=Adam(1e-3), loss=loss_fn, metrics=[\"mae\",\"mape\"])\n",
    "    return m\n",
    "\n",
    "def train_model(\n",
    "    model: Sequential,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    out_dir: Path,\n",
    "    epochs: int = 100,\n",
    "    batch_size: int = 128,\n",
    "    sample_weight: Optional[np.ndarray] = None,\n",
    ") -> tf.keras.callbacks.History:\n",
    "    ensure_dir(out_dir)\n",
    "    cb = [\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.5, min_lr=1e-5, verbose=1),\n",
    "        ModelCheckpoint(out_dir / \"best_model.keras\", monitor=\"val_loss\", mode=\"min\", save_best_only=True),\n",
    "        CSVLogger(out_dir / \"training_log.csv\", append=True),\n",
    "    ]\n",
    "    print(f\"[train] X={X.shape} y={y.shape} epochs={epochs} batch={batch_size}\")\n",
    "    hist = model.fit(\n",
    "        X, y, epochs=epochs, batch_size=batch_size, validation_split=0.1,\n",
    "        callbacks=cb, verbose=1, sample_weight=sample_weight\n",
    "    )\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de378d",
   "metadata": {},
   "source": [
    "## Wykresy i metryki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c903983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_curves(history: tf.keras.callbacks.History, out_dir: Path, tag: str) -> None:\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1); plt.plot(history.history[\"loss\"]); plt.plot(history.history[\"val_loss\"]); plt.title(\"Loss\"); plt.legend([\"train\",\"val\"])\n",
    "    plt.subplot(1,2,2); plt.plot(history.history[\"mae\"]);  plt.plot(history.history[\"val_mae\"]);  plt.title(\"MAE\");  plt.legend([\"train\",\"val\"])\n",
    "    plt.tight_layout()\n",
    "    path = out_dir / f\"training_curves_{tag}.png\"\n",
    "    plt.savefig(path, dpi=150); plt.close()\n",
    "    print(f\"[plot] {path}\")\n",
    "\n",
    "def plot_pred_diff_interactive(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    X: Optional[np.ndarray] = None,\n",
    "    in_cols: Optional[List[str]] = None,\n",
    "    out_dir: Path = Path(\".\"),\n",
    "    tag: str = \"plot\",\n",
    ") -> Dict[str, float]:\n",
    "    diffs = y_pred - y_true\n",
    "    win   = ROLLING_WINDOW  # stałe okno jak w oryginale\n",
    "    if X is not None and in_cols and ACTIVE_COMP_COL in in_cols:\n",
    "        idx = in_cols.index(ACTIVE_COMP_COL)\n",
    "        active = X[:, idx, 0] != 0\n",
    "        filt = np.where(active, diffs, np.nan)\n",
    "        roll = pd.Series(filt).ffill().rolling(window=win).mean()\n",
    "        roll_name = f\"rolling({win}) [active]\"\n",
    "    else:\n",
    "        roll = pd.Series(diffs).rolling(window=win).mean()\n",
    "        roll_name = f\"rolling({win})\"\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred); rmse = float(np.sqrt(mse))\n",
    "    mae = mean_absolute_error(y_true, y_pred); r2 = r2_score(y_true, y_pred)\n",
    "    fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Truth vs Pred\", \"Diff + Rolling\"))\n",
    "    fig.add_trace(go.Scatter(y=y_true, mode=\"lines\", name=\"true\"), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(y=y_pred, mode=\"lines\", name=\"pred\"), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(y=diffs, mode=\"lines\", name=\"diff\"), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(y=roll,  mode=\"lines\", name=roll_name), row=2, col=1)\n",
    "    fig.update_layout(height=800, title_text=f\"Prediction analysis — {tag}\", showlegend=True)\n",
    "    path = out_dir / f\"prediction_analysis_{tag}.html\"\n",
    "    fig.write_html(path); print(f\"[plot] {path}\")\n",
    "    return dict(mse=float(mse), rmse=rmse, mae=float(mae), r2=float(r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26f4b9",
   "metadata": {},
   "source": [
    "## Główny pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2f81921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_pipeline(epochs: int = 5, batch_size: int = 128):\n",
    "    set_seed(50)\n",
    "    ensure_dir(TRAIN_DIR); ensure_dir(TEST_DIR)\n",
    "    out_dir = make_session_dir(OUT_BASE)\n",
    "    save_feature_config(INPUT_COLUMNS, OUTPUT_COLUMN, out_dir)\n",
    "\n",
    "    train_files = sorted([f for f in TRAIN_DIR.glob(\"*.csv\")])\n",
    "    test_files  = [TEST_DIR / f for f in TEST_FILES_OVERRIDE] if TEST_FILES_OVERRIDE else sorted([f for f in TEST_DIR.glob(\"*.csv\")])\n",
    "    if not train_files:\n",
    "        print(f(\"[warn] wrzuć CSV do: {TRAIN_DIR}\")); return\n",
    "    if not test_files:\n",
    "        print(f(\"[warn] wrzuć CSV do: {TEST_DIR}\"))  # dalej i tak potrenuje, ale bez testu\n",
    "\n",
    "    scaler, out_scaler = StandardScaler(), StandardScaler()\n",
    "    enc = {\"onehot\": _init_onehot_encoder()}\n",
    "    model, used_cols = None, INPUT_COLUMNS\n",
    "\n",
    "    all_metrics = []\n",
    "    for i, fpath in enumerate(train_files, start=1):\n",
    "        df = read_csv_auto(fpath)\n",
    "        X_tr, y_tr, scaler, out_scaler, enc = normalize_data(\n",
    "            df, used_cols, OUTPUT_COLUMN, scaler, out_scaler, enc, fit_encoders=(i==1)\n",
    "        )\n",
    "        if X_tr is None:\n",
    "            print(f\"[train] skip {fpath.name}\"); continue\n",
    "\n",
    "        if model is None:\n",
    "            model = build_model(X_tr.shape[1]); model.summary()\n",
    "\n",
    "        # opcjonalne dociążenie próbek aktywnych\n",
    "        sample_weight = None\n",
    "        if ACTIVE_WEIGHT_ALPHA > 0 and ACTIVE_COMP_COL in used_cols:\n",
    "            idx_act = used_cols.index(ACTIVE_COMP_COL)\n",
    "            active  = (X_tr[:, idx_act, 0] != 0).astype(np.float32)\n",
    "            sample_weight = 1.0 + ACTIVE_WEIGHT_ALPHA * active\n",
    "\n",
    "        hist = train_model(model, X_tr, y_tr, out_dir, epochs=epochs, batch_size=batch_size, sample_weight=sample_weight)\n",
    "        pd.DataFrame(hist.history).to_csv(out_dir / f\"history_train_{i:02d}.csv\", index=False)\n",
    "        plot_training_curves(hist, out_dir, tag=f\"train_{i:02d}\")\n",
    "        save_scalers(scaler, out_scaler, enc, out_dir)\n",
    "\n",
    "        # snapshot modelu po etapie\n",
    "        model.save(out_dir / f\"model_iter_{i:02d}.keras\")\n",
    "        (out_dir / f\"model_iter_{i:02d}_config.json\").write_text(\n",
    "            json.dumps({\"input_shape\": int(X_tr.shape[1])}), encoding=\"utf-8\"\n",
    "        )\n",
    "\n",
    "        # test po każdej iteracji\n",
    "        for j, tpath in enumerate(test_files, start=1):\n",
    "            if not tpath.exists():\n",
    "                print(f\"[test] missing {tpath.name}\"); continue\n",
    "            df_t = read_csv_auto(tpath)\n",
    "            X_te, y_te, _, _, _ = normalize_data(df_t, used_cols, OUTPUT_COLUMN, scaler, out_scaler, enc, fit_encoders=False)\n",
    "            if X_te is None:\n",
    "                print(f\"[test] skip {tpath.name}\"); continue\n",
    "\n",
    "            y_pred_n = model.predict(X_te, verbose=0).flatten()\n",
    "            y_true = out_scaler.inverse_transform(y_te.reshape(-1,1)).flatten()\n",
    "            y_pred = out_scaler.inverse_transform(y_pred_n.reshape(-1,1)).flatten()\n",
    "\n",
    "            m = plot_pred_diff_interactive(y_true, y_pred, X=X_te, in_cols=used_cols, out_dir=out_dir, tag=f\"tr{i:02d}_te{j:02d}\")\n",
    "            row = {\"train_file\": fpath.name, \"test_file\": tpath.name, **m}\n",
    "            all_metrics.append(row)\n",
    "            print(f\"[metrics] {tpath.name}: MSE={m['mse']:.3f} RMSE={m['rmse']:.3f} MAE={m['mae']:.3f} R2={m['r2']:.3f}\")\n",
    "\n",
    "    if all_metrics:\n",
    "        pd.DataFrame(all_metrics).to_csv(out_dir / \"test_metrics.csv\", index=False)\n",
    "        print(f\"[eval] metrics → {out_dir / 'test_metrics.csv'}\")\n",
    "    print(f\"[done] artifacts → {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481cce48",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4794ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[seed] 50\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 13, 64)            128       \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 13, 128)           8320      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 128)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 13, 64)            8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 13, 64)            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 13, 256)          148992    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 13, 256)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              123648    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 289,473\n",
      "Trainable params: 289,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[train] X=(1500, 13, 1) y=(1500, 1) epochs=5 batch=128\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 13s 258ms/step - loss: 1.5051 - mae: 0.7777 - mape: 131.8344 - val_loss: 1.2225 - val_mae: 0.6538 - val_mape: 101.0797 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 1.0629 - mae: 0.6068 - mape: 290.1432 - val_loss: 0.8385 - val_mae: 0.5056 - val_mape: 149.2034 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 0.7912 - mae: 0.4971 - mape: 313.3864 - val_loss: 0.6544 - val_mae: 0.4307 - val_mape: 104.6771 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.6027 - mae: 0.4115 - mape: 215.8571 - val_loss: 0.4532 - val_mae: 0.3049 - val_mape: 99.5847 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.4689 - mae: 0.3340 - mape: 160.2973 - val_loss: 0.3664 - val_mae: 0.2580 - val_mape: 88.7124 - lr: 0.0010\n",
      "[plot] C:\\Users\\admin\\Desktop\\Nowy folder (3)\\OUTPUTS\\2025-09-17_08-37-46\\training_curves_train_01.png\n",
      "[plot] C:\\Users\\admin\\Desktop\\Nowy folder (3)\\OUTPUTS\\2025-09-17_08-37-46\\prediction_analysis_tr01_te01.html\n",
      "[metrics] sample_test_A.csv: MSE=0.043 RMSE=0.207 MAE=0.165 R2=0.900\n",
      "[plot] C:\\Users\\admin\\Desktop\\Nowy folder (3)\\OUTPUTS\\2025-09-17_08-37-46\\prediction_analysis_tr01_te02.html\n",
      "[metrics] sample_test_B.csv: MSE=0.042 RMSE=0.204 MAE=0.165 R2=0.903\n",
      "[train] X=(1500, 13, 1) y=(1500, 1) epochs=5 batch=128\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 0.3990 - mae: 0.3117 - mape: 152.3398 - val_loss: 0.3116 - val_mae: 0.2280 - val_mape: 128.3896 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 0.3525 - mae: 0.2950 - mape: 127.8053 - val_loss: 0.2823 - val_mae: 0.2259 - val_mape: 128.2134 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 0.3268 - mae: 0.2922 - mape: 121.3693 - val_loss: 0.2578 - val_mae: 0.2227 - val_mape: 111.7852 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 0.3121 - mae: 0.2991 - mape: 133.4975 - val_loss: 0.2388 - val_mae: 0.2213 - val_mape: 121.5360 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 0.2876 - mae: 0.2921 - mape: 142.5767 - val_loss: 0.2273 - val_mae: 0.2302 - val_mape: 110.9749 - lr: 0.0010\n",
      "[plot] C:\\Users\\admin\\Desktop\\Nowy folder (3)\\OUTPUTS\\2025-09-17_08-37-46\\training_curves_train_02.png\n",
      "[plot] C:\\Users\\admin\\Desktop\\Nowy folder (3)\\OUTPUTS\\2025-09-17_08-37-46\\prediction_analysis_tr02_te01.html\n",
      "[metrics] sample_test_A.csv: MSE=0.043 RMSE=0.207 MAE=0.163 R2=0.900\n",
      "[plot] C:\\Users\\admin\\Desktop\\Nowy folder (3)\\OUTPUTS\\2025-09-17_08-37-46\\prediction_analysis_tr02_te02.html\n",
      "[metrics] sample_test_B.csv: MSE=0.043 RMSE=0.207 MAE=0.167 R2=0.900\n",
      "[eval] metrics → C:\\Users\\admin\\Desktop\\Nowy folder (3)\\OUTPUTS\\2025-09-17_08-37-46\\test_metrics.csv\n",
      "[done] artifacts → C:\\Users\\admin\\Desktop\\Nowy folder (3)\\OUTPUTS\\2025-09-17_08-37-46\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ustal parametry i odpal\n",
    "EPOCHS = 5\n",
    "BATCH  = 128\n",
    "run_pipeline(epochs=EPOCHS, batch_size=BATCH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
